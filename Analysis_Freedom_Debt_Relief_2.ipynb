{"cells":[{"cell_type":"markdown","metadata":{"id":"BvXkEf9FRiAw"},"source":["# **SENTIMENT ANALYSIS OF FREEDOM DEPT RELIEF COMPANY**\n","# **REVIEWS IN TRUSTPILOT WEBSITE**"]},{"cell_type":"markdown","metadata":{"id":"IBRO0RkxSNsz"},"source":["![image.png](attachment:image.png)\n","\n","https://www.trustpilot.com/review/freedomdebtrelief.com"]},{"cell_type":"markdown","metadata":{"id":"cATpMArqxccp"},"source":["#### IMPORT LIBRARIES AND DATASET"]},{"cell_type":"code","source":["!pip install transformers\n","!pip install emoji"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zNwp3SwYp_Ag","executionInfo":{"status":"ok","timestamp":1698687426147,"user_tz":-180,"elapsed":8327,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}},"outputId":"ea3e281a-2b2a-4070-8fed-ffd8cbfad5ae"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: emoji in /usr/local/lib/python3.10/dist-packages (2.8.0)\n"]}]},{"cell_type":"code","source":["!pip install --upgrade tensorflow"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"TlvQwQFTqmnd","executionInfo":{"status":"ok","timestamp":1698687574044,"user_tz":-180,"elapsed":57446,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}},"outputId":"ed6dfb98-50ea-41f9-e13b-5c73e5640f45"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n","Collecting tensorflow\n","  Downloading tensorflow-2.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (489.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.8/489.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n","Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.0)\n","Collecting tensorboard<2.15,>=2.14 (from tensorflow)\n","  Downloading tensorboard-2.14.1-py3-none-any.whl (5.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow)\n","  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting keras<2.15,>=2.14.0 (from tensorflow)\n","  Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n","Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.15,>=2.14->tensorflow)\n","  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.7.22)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n","Installing collected packages: tensorflow-estimator, keras, google-auth-oauthlib, tensorboard, tensorflow\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.12.0\n","    Uninstalling tensorflow-estimator-2.12.0:\n","      Successfully uninstalled tensorflow-estimator-2.12.0\n","  Attempting uninstall: keras\n","    Found existing installation: keras 2.12.0\n","    Uninstalling keras-2.12.0:\n","      Successfully uninstalled keras-2.12.0\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 0.4.6\n","    Uninstalling google-auth-oauthlib-0.4.6:\n","      Successfully uninstalled google-auth-oauthlib-0.4.6\n","  Attempting uninstall: tensorboard\n","    Found existing installation: tensorboard 2.12.0\n","    Uninstalling tensorboard-2.12.0:\n","      Successfully uninstalled tensorboard-2.12.0\n","  Attempting uninstall: tensorflow\n","    Found existing installation: tensorflow 2.12.0\n","    Uninstalling tensorflow-2.12.0:\n","      Successfully uninstalled tensorflow-2.12.0\n","Successfully installed google-auth-oauthlib-1.0.0 keras-2.14.0 tensorboard-2.14.1 tensorflow-2.14.0 tensorflow-estimator-2.14.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["keras","tensorboard","tensorflow"]}}},"metadata":{}}]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495},"executionInfo":{"elapsed":2047,"status":"error","timestamp":1698687600424,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"},"user_tz":-180},"id":"PHHa63S7NpXC","outputId":"194f23c2-d6cd-4167-fc1d-49c170bd8893"},"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1283\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquestion_answering\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQuestionAnsweringArgumentHandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuestionAnsweringPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtable_question_answering\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTableQuestionAnsweringArgumentHandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTableQuestionAnsweringPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtext2text_generation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummarizationPipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mText2TextGenerationPipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTranslationPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/table_question_answering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_probability/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# from tensorflow_probability.google import tfp_google  # DisableOnExport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# pylint: disable=wildcard-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_probability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mpkg_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_maybe_nonlazy_load\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Forces loading the package from its lazy loader.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/internal/lazy_loader.py\u001b[0m in \u001b[0;36m__dir__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/internal/lazy_loader.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_first_access\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_first_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_first_access\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py\u001b[0m in \u001b[0;36m_validate_tf_environment\u001b[0;34m(package)\u001b[0m\n\u001b[1;32m     58\u001b[0m       distutils.version.LooseVersion(required_tensorflow_version)):\n\u001b[0;32m---> 59\u001b[0;31m     raise ImportError(\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;34m'This version of TensorFlow Probability requires TensorFlow '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: This version of TensorFlow Probability requires TensorFlow version >= 2.14; Detected an installation of version 2.12.0. Please upgrade TensorFlow to proceed.","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-eb58b031adaa>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1273\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1282\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m   1285\u001b[0m                 \u001b[0;34mf\"Failed to import {self.__name__}.{module_name} because of the following error (look up to see its\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1286\u001b[0m                 \u001b[0;34mf\" traceback):\\n{e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.pipelines because of the following error (look up to see its traceback):\nThis version of TensorFlow Probability requires TensorFlow version >= 2.14; Detected an installation of version 2.12.0. Please upgrade TensorFlow to proceed."]}],"source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import matplotlib.font_manager as fm\n","import re\n","\n","\n","\n","\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import BertTokenizer, BertForSequenceClassification\n","\n","from wordcloud import WordCloud\n","from collections import Counter\n","\n","import emoji\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","\n","from gensim import corpora, models\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n","\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.linear_model import LinearRegression, LogisticRegression\n","from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n","from sklearn.svm import SVR, SVC\n","from sklearn.metrics import mean_squared_error, accuracy_score, classification_report\n","\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('vader_lexicon')"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"VUIt4P3zN6XR","colab":{"base_uri":"https://localhost:8080/","height":339},"executionInfo":{"status":"error","timestamp":1698687646981,"user_tz":-180,"elapsed":374,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}},"outputId":"35057982-3b07-4cfa-970f-f73918a05962"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-066aa3c711d3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Freedom_Debt_Relief.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Freedom_Debt_Relief.csv'"]}],"source":["df= pd.read_csv('Freedom_Debt_Relief.csv')"]},{"cell_type":"markdown","metadata":{"id":"Ls_1_Zrkxm25"},"source":["### DATA PREPROCESSING"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12,"status":"aborted","timestamp":1698685602050,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"},"user_tz":-180},"id":"WkAQHMhSPboa"},"outputs":[],"source":["df.sample(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oia-RHCLjJb-","executionInfo":{"status":"aborted","timestamp":1698685602051,"user_tz":-180,"elapsed":12,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["df.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XmXlsuSEjJb_","executionInfo":{"status":"aborted","timestamp":1698685602051,"user_tz":-180,"elapsed":12,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["# Remove \"review\" and \"reviews\" from the values in the \"Reviews Count\" column\n","df['Reviews Count'] = df['Reviews Count'].str.replace('reviews', '').str.replace('review', '')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5U4BSiKzjJcA","executionInfo":{"status":"aborted","timestamp":1698685602052,"user_tz":-180,"elapsed":12,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["# Convert the \"Reply Date\" column to a string data type\n","df['Reply Date'] = df['Reply Date'].astype(str)\n","\n","# Remove \"Reply from Freedom Debt Relief\" from the \"Reply Date\" column in the existing DataFrame\n","df['Reply Date'] = df['Reply Date'].str.replace(\"Reply from Freedom Debt Relief\", \"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7cgXCBsZjJcC","executionInfo":{"status":"aborted","timestamp":1698685602053,"user_tz":-180,"elapsed":13,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["# Convert 'Experience Date', 'Review Date', 'Reply Date' columns to date\n","df[['Experience Date', 'Review Date', 'Reply Date']] = df[['Experience Date', 'Review Date', 'Reply Date']].apply(pd.to_datetime, errors='coerce')\n","\n","# Convert \"Reviews Count\" column to integer\n","df['Reviews Count'] = pd.to_numeric(df['Reviews Count'], errors='coerce').astype('Int64')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"celd9znnjJcC","executionInfo":{"status":"aborted","timestamp":1698685602054,"user_tz":-180,"elapsed":13,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["df.sample(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"aborted","timestamp":1698685602054,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"},"user_tz":-180},"id":"NjKXS9PgPflC"},"outputs":[],"source":["df.info()"]},{"cell_type":"markdown","metadata":{"id":"1HWpe9VEaMG-"},"source":["#### MISSING & UNIQUE VALUES"]},{"cell_type":"markdown","metadata":{"id":"p3KgygHydpY2"},"source":["**Missing Values**"]},{"cell_type":"markdown","metadata":{"id":"ThHcK_lTjJcF"},"source":["### Imputation of missing values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R8g_St3XjJcF","executionInfo":{"status":"aborted","timestamp":1698685602055,"user_tz":-180,"elapsed":14,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["df['Reviewer Name'].fillna('Customerxxx', inplace=True)\n","df['Review Title'].fillna('Review for Freedom Dept Relief', inplace=True)\n","df['Review Date'].fillna(df['Experience Date'], inplace=True)\n","df.isna().sum()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1CF-s-MtjJcG","executionInfo":{"status":"aborted","timestamp":1698685602055,"user_tz":-180,"elapsed":13,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["df.info()"]},{"cell_type":"markdown","metadata":{"id":"EoW0kH8ZWjep"},"source":["**Unique Values**"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"aborted","timestamp":1698685602055,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"},"user_tz":-180},"id":"ybsmm7TiVewf"},"outputs":[],"source":["df.nunique()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":13,"status":"aborted","timestamp":1698685602056,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"},"user_tz":-180},"id":"d8IxDqbiyZWq"},"outputs":[],"source":["columns_to_plot = [\"Reviews Count\", \"Country Code\", \"Experience Date\", \"Rating\", \"Review Date\", \"Reply Date\"]\n","\n","# Create a DataFrame with the unique value counts for the selected columns\n","unique_value_counts = df[columns_to_plot].nunique()\n","\n","# Create the bar plot\n","plt.figure(figsize=(12, 6))\n","ax = sns.barplot(x=unique_value_counts.index, y=unique_value_counts.values, hue=unique_value_counts.index, legend=False, palette='viridis')\n","plt.xticks()\n","plt.xlabel('Columns', weight='bold')\n","plt.ylabel('Number of Unique Values', weight='bold')\n","plt.title('Number of Unique Values for Non Text Column', weight='bold')\n","plt.tight_layout()\n","\n","# Annotate the bars with the number of unique values\n","for p in ax.patches:\n","    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n","                ha='center', va='center', fontsize=10, color='black', weight='bold', xytext=(0, 5),\n","                textcoords='offset points')\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"BHH_ijnGn5cc"},"source":["### Descriptive Statistics"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"aborted","timestamp":1698685602057,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"},"user_tz":-180},"id":"5IG3uHe8qFXI"},"outputs":[],"source":["# Basic statistics\n","numeric_stats = df[['Reviews Count', 'Rating']].describe()\n","numeric_stats"]},{"cell_type":"markdown","metadata":{"id":"nxNdKpj8jJcK"},"source":["### Data Visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u1KKLIBEjJcL","executionInfo":{"status":"aborted","timestamp":1698685602057,"user_tz":-180,"elapsed":12108,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["from wordcloud import WordCloud\n","\n","# Generate the word cloud\n","wordcloud = WordCloud(width=800, height=400).generate(' '.join(df['Review Text'].dropna()))\n","\n","# Create a figure with the specified title\n","plt.figure(figsize=(10, 5))\n","plt.imshow(wordcloud, interpolation='bilinear')\n","plt.axis('off')\n","\n","# Set the title with custom font size and weight\n","plt.title('Word Cloud of Review Text', fontsize=14, fontweight='bold')\n","\n","# Show the word cloud\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"1U2CfjNtjJcL"},"source":["## RATING ANALYSIS"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JGNRwEHsjJcL","executionInfo":{"status":"aborted","timestamp":1698685602057,"user_tz":-180,"elapsed":12099,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Extract the year from the 'Review Date' column\n","df['Year'] = df['Review Date'].dt.year\n","\n","# Group the data by year and calculate the count of ratings\n","yearly_rating_counts = df.groupby(['Year', 'Rating'])['Rating'].count().unstack().fillna(0)\n","\n","# Set the color map for rating categories\n","colors = plt.cm.Paired(np.arange(5))\n","\n","# Create a larger figure\n","fig, ax = plt.subplots(figsize=(12, 6))\n","\n","# Define the width of each bar\n","bar_width = 0.15\n","\n","# Define the x positions for the bars\n","x = yearly_rating_counts.index\n","\n","# Create bar plots for each rating category\n","for i, (rating, color) in enumerate(zip(yearly_rating_counts.columns, colors)):\n","    ratings = yearly_rating_counts[rating]\n","    ax.bar(x + (i * bar_width), ratings, bar_width, label=f'Rating {rating}', color=color)\n","\n","# Set x-axis labels\n","ax.set_xticks(x)\n","ax.set_xticklabels(x, rotation=45)  # Rotate year labels for better readability\n","\n","# Set y-axis label\n","ax.set_ylabel('Number of Ratings')\n","\n","# Set a title for the bar plot\n","ax.set_title('Ratings Distribution Over the Years')\n","\n","# Add a legend\n","ax.legend()\n","\n","# Show the bar plot\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"8d9CVGfgjJcM"},"source":["**The distribution of ratings for Freedom Dept Relief Company over the years. Here are some key observations and comments:**\n","\n","1. **Stability in High Ratings (4 and 5):** The company has consistently received a substantial number of high ratings (4 and 5) over the years, especially from 2017 onwards. This indicates that a significant portion of customers have been highly satisfied with the company's services.\n","\n","2. **Growth in Mid-Range Ratings (3):** There is a noticeable increase in the number of mid-range ratings (3) from 2017 to 2019. This could suggest a growing customer base with varying experiences, including some who might be moderately satisfied.\n","\n","3. **Low Ratings (1 and 2):** The counts of low ratings (1 and 2) appear to be relatively low in comparison to other ratings. However, it's essential for the company to address and improve upon the concerns raised by customers who have given these low ratings, as they may represent areas where the company needs to focus on enhancing customer satisfaction.\n","\n","4. **Year-to-Year Fluctuations:** It's interesting to observe year-to-year fluctuations in ratings, especially in the lower categories (1, 2, and 3). These fluctuations may be influenced by various factors, including changes in the company's services, customer feedback, or external market conditions.\n","\n","5. **Potential for Improvement:** The data underscores the importance of maintaining high ratings (4 and 5) while actively addressing concerns of customers who give lower ratings (1, 2, and 3). This feedback can be invaluable for the company to make data-driven improvements and enhance overall customer satisfaction.\n","\n","In summary, analyzing the distribution of ratings by year provides insights into the company's performance and customer sentiment over time. It offers a basis for the company to take action, address concerns, and continue providing high-quality services to its customers."]},{"cell_type":"markdown","metadata":{"id":"G0TmS2RvjJcM"},"source":["b. Time-Series Decomposition: Decompose time series data into trend, seasonal, and residual components to identify patterns and anomalies."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JSmrsbscjJcM","executionInfo":{"status":"aborted","timestamp":1698685602058,"user_tz":-180,"elapsed":12091,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Extract the year from the 'Review Date' column\n","df['Year'] = df['Review Date'].dt.year\n","\n","# Group the data by year and calculate the count of ratings\n","yearly_rating_counts = df.groupby('Year')['Rating'].value_counts().unstack().fillna(0)\n","\n","# Create a color palette for the pie charts\n","colors = plt.cm.Paired(range(5))\n","\n","# Create a larger figure with multiple subplots\n","fig, axes = plt.subplots(5, 2, figsize=(12, 18), gridspec_kw={'hspace': 0.6, 'wspace': 0.4})\n","\n","# Flatten the 2D array of subplots for easier indexing\n","axes = np.ravel(axes)\n","\n","# Set the radius for the overall pie\n","overall_pie_radius = 1.5\n","\n","for i, (year, ratings) in enumerate(yearly_rating_counts.iterrows()):\n","    # Create a pie chart with custom text properties\n","    wedges, texts, autotexts = axes[i].pie(\n","        ratings, labels=None, startangle=90, colors=colors, pctdistance=1.1, autopct='%1.1f%%',\n","        textprops={'size': 8}  # Set font size to 8 points\n","    )\n","\n","    # Set the radius for the overall pie\n","    for wedge in wedges:\n","        wedge.set_radius(overall_pie_radius)\n","\n","    # Set a title for each pie chart above it\n","    axes[i].set_title(f'Ratings Distribution for {year}', fontsize=14, weight='bold', y=1.1)\n","\n","    # Equal aspect ratio ensures that the pie is drawn as a circle\n","    axes[i].axis('equal')\n","\n","    # Print the values for each rating category for this year\n","    for rating, count in ratings.items():\n","        print(f'Year: {year}, Rating: {rating}, Count: {count}, Percentage: {count / ratings.sum() * 100:.1f}%')\n","\n","# Adjust spacing between subplots\n","plt.tight_layout()\n","\n","# Adjust title positions to avoid overlapping\n","for ax in axes:\n","    ax.title.set_y(1.15)  # Adjust the vertical position of titles\n","\n","# Show the pie charts\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"6rl8BMaNjJcN"},"source":["**The distribution of ratings by year for Freedom Dept Relief Company, with the ratings categorized into five levels (1 to 5). Let's discuss and conclude based on the provided information:**\n","\n","1. **Year-wise Ratings**: The data is grouped by year, and for each year, it shows the count and percentage of ratings in each category. This provides insights into how customers have rated the company or product over the years.\n","\n","2. **Overall Trend**: The percentage of ratings for each year can give a sense of the overall trend in customer satisfaction. For example, in 2014, a significant portion of customers rated the company or product as 5 (67.8%), while in 2017, a higher percentage of customers rated it as 5 (76.8%). This suggests an improvement in customer satisfaction.\n","\n","3. **Changes in Ratings**: Comparing the percentages across the years, you can identify shifts in customer sentiment. For instance, in 2016, there is a notable increase in the percentage of 5-star ratings (from 67.8% to 71.7%), indicating an improved reputation. In contrast, the percentage of 1-star ratings decreases over the years.\n","\n","4. **Variability**: The data also reveals the variability in ratings. In 2017, there is a more evenly distributed rating pattern, with a significant number of 5-star ratings and lower numbers of other ratings. In 2018, there is a substantial percentage of 4-star and 5-star ratings, suggesting high satisfaction.\n","\n","5. **Management Insights**: Based on this data, company owners and managers can draw several insights:\n","\n","   - **Yearly Performance**: They can assess the performance and satisfaction of customers on a yearly basis. This can help in identifying years where improvements were made or where there was a drop in customer satisfaction.\n","\n","   - **Areas for Improvement**: For years with lower satisfaction, it's essential to dig deeper to understand the reasons behind the lower ratings. Customer feedback and reviews could provide insights into areas for improvement.\n","\n","   - **Customer Engagement**: The data can be used to analyze the impact of changes in products or services on customer ratings. Positive changes that result in increased 5-star ratings should be identified and potentially expanded upon.\n","\n","   - **Targeted Marketing**: Understanding customer sentiment over time can inform marketing strategies. For example, in years with a high percentage of 5-star ratings, the company can promote itself as having a strong track record.\n","\n","6. **Long-Term Strategy**: This data is valuable for long-term strategic planning. Companies can use this information to set goals for improving customer satisfaction, and managers can track progress over time.\n","\n","7. **Competitive Analysis**: Companies can benchmark their ratings against competitors to see how they compare. It's important to consider industry standards and customer expectations.\n","\n","In conclusion, the data provides valuable insights into the company's performance and customer satisfaction over the years. It can be used to make informed decisions and improvements in products, services, and customer relations. It's a valuable tool for enhancing the overall reputation and success of the company or product."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OQIa6aCMjJcO","executionInfo":{"status":"aborted","timestamp":1698685602058,"user_tz":-180,"elapsed":12081,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["from statsmodels.tsa.seasonal import seasonal_decompose\n","import matplotlib.pyplot as plt\n","\n","# Perform seasonal decomposition\n","result = seasonal_decompose(df['Rating'], model='additive', period=365)\n","\n","# Create a figure with a grid layout\n","fig, axes = plt.subplots(4, 1, figsize=(12, 8), sharex=True)\n","\n","# Custom color palette for better readability\n","colors = ['blue', 'green', 'red', 'black']\n","\n","# Titles for each component\n","titles = ['Trend Component', 'Seasonal Component', 'Residual Component', 'Observed']\n","\n","for i, (ax, title, color) in enumerate(zip(axes, titles, colors)):\n","    # Customize the appearance of each component\n","    component = result.trend if i == 0 else [result.seasonal, result.resid, df['Rating']][i - 1]\n","    component.plot(ax=ax, title=title, color=color, legend=False)\n","\n","    # Set title font properties\n","    ax.set_title(title, fontsize=13, fontweight='bold')\n","\n","    ax.set_ylabel('Rating' if i == 3 else '')  # Label only the last subplot\n","\n","# Set common x-axis label\n","plt.xlabel('Date')\n","\n","# Fine-tune the layout\n","plt.tight_layout()\n","\n","# Print details of each component\n","for title, component in zip(titles, [result.trend, result.seasonal, result.resid, df['Rating']]):\n","    print(f\"{title} Summary:\")\n","    print(component.describe())\n","    print(\"\\n\")\n","\n","# Show the plot\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"LBCiY50IjJcP"},"source":["The summary statistics for the Trend, Seasonal, Residual, and Observed components obtained from the seasonal decomposition of the 'Rating' data provide valuable insights into the underlying patterns and variations in the dataset.\n","\n","Let's discuss and conclude the findings for each component:\n","\n","**Trend Component:**\n","- The trend component represents the underlying trend in the 'Rating' data.\n","- It shows a positive trend in ratings over time, with a mean rating of approximately 4.6024.\n","- The trend component exhibits relatively low variability, indicating a consistent upward trend.\n","\n","**Seasonal Component:**\n","- The seasonal component captures recurring patterns in the 'Rating' data.\n","- Seasonal variations mostly balance out over time, as the mean seasonal rating is close to zero.\n","- The seasonal component has a moderate level of variability, showing regular but not extreme seasonal patterns.\n","\n","**Residual Component:**\n","- The residual component contains unexplained variations in the 'Rating' data after removing trend and seasonality.\n","- The mean residual rating is close to zero, indicating effective removal of trend and seasonality.\n","- The residual component exhibits relatively high variability, suggesting the presence of outliers or unpredictable fluctuations.\n","\n","**Observed Component:**\n","- The observed component represents the original 'Rating' data.\n","- The mean observed rating is approximately 4.6022, indicating that the seasonal decomposition process effectively smooths out most variations.\n","- The original ratings have relatively high variability, with a standard deviation of 0.9210.\n","\n","**Conclusion:**\n","- The 'Rating' data shows a positive trend over time.\n","- Seasonal variations are noticeable but are mostly balanced out over the period.\n","- The residual component represents unexplained variations and may contain outliers.\n","- The seasonal decomposition effectively captures trends and seasonal patterns in the data while retaining the original variability.\n","\n","This concise summary provides a quick overview of the main findings without delving into extensive numeric details.\n"]},{"cell_type":"markdown","metadata":{"id":"jSNgTq2sjJcP"},"source":["### Aggregation and Grouping:\n","\n","a. Group Data by Time Period: Aggregate data by specific time intervals (e.g., monthly, quarterly) to analyze trends and patterns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xClY7bF7jJcQ","executionInfo":{"status":"aborted","timestamp":1698685602058,"user_tz":-180,"elapsed":12071,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Group data by quarter and calculate the mean rating for each quarter\n","quarterly_mean_rating = df.groupby(df['Review Date'].dt.to_period('Q')).agg({'Rating': 'mean'})\n","\n","# Create a bar plot for quarterly mean rating\n","plt.figure(figsize=(12, 6))\n","ax = sns.barplot(x=quarterly_mean_rating.index.strftime('%b %Y'), y=quarterly_mean_rating['Rating'], color='lightblue')\n","\n","# Add labels and titles\n","plt.title('Quarterly Mean Rating', fontsize=16)\n","plt.xlabel('Quarter', fontsize=14)\n","plt.ylabel('Mean Rating', fontsize=14)\n","\n","# Rotate x-axis labels for readability (rotate 90 degrees)\n","plt.xticks(rotation=90, ha=\"right\", fontsize=12)\n","\n","# Display data values on top of bars\n","for p in ax.patches:\n","    ax.annotate(f'{p.get_height():.2f}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=10, color='black', xytext=(0, 5), textcoords='offset points')\n","\n","# Show the plot\n","plt.tight_layout()\n","plt.show()\n","\n","# Print information about the quarterly mean rating\n","print(\"Quarterly Mean Rating:\")\n","print(quarterly_mean_rating)\n"]},{"cell_type":"markdown","metadata":{"id":"I_m7z9zTjJcQ"},"source":["**Ratings by quarterly time intervals, allowing us to analyze trends and patterns over time. Let's discuss and conclude the information presented:**\n","\n","**Quarterly Mean Rating:**\n","\n","The table provides the mean rating for each quarter starting from 2014Q1 up to 2023Q4. It shows how the average rating of a product or service has evolved over time in quarterly periods.\n","\n","Here are the key observations and conclusions:\n","\n","1. **Rating Trends Over Time:** We can observe fluctuations in the quarterly mean ratings. Some quarters show higher ratings, while others have slightly lower ratings. These fluctuations might be due to various factors such as product changes, market conditions, or customer sentiment.\n","\n","2. **Seasonality:** There might be seasonality in the ratings. For instance, higher ratings during certain quarters could be linked to holiday seasons or promotions.\n","\n","3. **Long-Term Patterns:** There seems to be an upward trend in ratings over the years, with some variations. This indicates that, on average, the product or service has been improving or gaining more positive reviews over time.\n","\n","4. **Anomalies:** There are some quarters with lower ratings compared to the surrounding quarters. Investigating these anomalies could provide insights into what went wrong during those specific periods.\n","\n","5. **Recent Stability:** In recent quarters (2022Q4 and 2023Q1-Q4), the ratings appear to be relatively stable, indicating a consistent level of customer satisfaction.\n","\n","6. **Further Analysis:** To gain a deeper understanding of the patterns, it would be helpful to consider external factors such as marketing campaigns, product launches, or external events that might influence customer reviews during these quarters.\n","\n","In conclusion, analyzing the quarterly mean ratings is valuable for tracking trends, understanding seasonality, and assessing long-term changes in customer sentiment. Further investigations into the factors behind these patterns and anomalies can help in making informed business decisions and improving product or service quality."]},{"cell_type":"markdown","metadata":{"id":"QTKpozawjJcR"},"source":["### Time-Related Calculations:\n","\n","a. Calculate Time Difference: Calculate the time difference between two date columns. For instance, you can calculate the response time by subtracting the review date from the reply date."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CbTvCWmZjJcR","executionInfo":{"status":"aborted","timestamp":1698685602059,"user_tz":-180,"elapsed":12070,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["df['Review Response'] = df['Review Date'] - df['Experience Date']\n","\n","df['Reply Response'] = df['Reply Date'] - df['Review Date']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u8j8-IQkjJcR","executionInfo":{"status":"aborted","timestamp":1698685602059,"user_tz":-180,"elapsed":12063,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["df.head()"]},{"cell_type":"markdown","metadata":{"id":"_M5nOkm4jJca"},"source":["b. Extract Date Components: Extract specific components from the date, such as year, month, day, or weekday."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jq_GM4LRjJca","executionInfo":{"status":"aborted","timestamp":1698685602059,"user_tz":-180,"elapsed":12061,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["df['Year'] = df['Review Date'].dt.year\n","df['Month'] = df['Review Date'].dt.month\n","df['Day'] = df['Review Date'].dt.day\n","df['Weekday'] = df['Review Date'].dt.day_name()"]},{"cell_type":"markdown","metadata":{"id":"qfcxJFmvjJcb"},"source":["#### Ratings Over Time (Monthly Average)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2wLoqPUmjJcb","executionInfo":{"status":"aborted","timestamp":1698685602060,"user_tz":-180,"elapsed":12053,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib.dates as mdates\n","import pandas as pd\n","\n","# Downsample the data by averaging ratings by month\n","monthly_ratings = df.resample('M', on='Review Date')['Rating'].mean().dropna()\n","\n","# Create a time series plot of downsampled ratings\n","plt.figure(figsize=(16, 6))\n","plt.plot(monthly_ratings.index, monthly_ratings.values, marker='o', linestyle='-', color='b')\n","plt.title('Ratings Over Time (Monthly Average)', fontsize=14, fontweight='bold')\n","plt.xlabel('Date', fontsize=14)\n","plt.ylabel('Average Rating', fontsize=14)\n","plt.grid(True)\n","\n","# Set x-axis tick locator and formatter for years only\n","years = mdates.YearLocator()\n","years_fmt = mdates.DateFormatter('%Y')  # Display years as four-digit numbers\n","\n","plt.gca().xaxis.set_major_locator(years)\n","plt.gca().xaxis.set_major_formatter(years_fmt)\n","\n","# Show the plot\n","plt.tight_layout()\n","plt.show()\n","\n","# Print information about the downsampled ratings\n","# Convert the monthly_ratings data to a DataFrame\n","monthly_ratings_df = monthly_ratings.reset_index()\n","\n","# Save the DataFrame to a CSV file\n","monthly_ratings_df.to_csv('monthly_ratings.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"AP-XE5ihjJcb"},"source":["### Visualization for Mean Rating by Year and Month:\n","\n","a. Calculate summary statistics for different time periods, such as the mean rating for each year or month."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9xaDhHF6jJcc","executionInfo":{"status":"aborted","timestamp":1698685602060,"user_tz":-180,"elapsed":12049,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["# Mean rating by year\n","mean_rating_by_year = df.groupby(df['Review Date'].dt.year)['Rating'].mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yLxhm_5gjJcc","executionInfo":{"status":"aborted","timestamp":1698685602060,"user_tz":-180,"elapsed":12040,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","# Calculate the mean rating by year\n","mean_rating_by_year = df.groupby(df['Review Date'].dt.year)['Rating'].mean()\n","\n","# Calculate the mean rating by month\n","mean_rating_by_month = df.groupby(df['Review Date'].dt.month)['Rating'].mean()\n","\n","# Print the results\n","print(\"Mean Rating by Year:\")\n","print(mean_rating_by_year)\n","\n","print(\"Mean Rating by Month:\")\n","print(mean_rating_by_month)\n","\n","# Create subplots for both graphs\n","fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n","\n","# Plot the mean rating by year\n","ax1.plot(mean_rating_by_year.index, mean_rating_by_year.values, marker='o', linestyle='-', color='b')\n","ax1.set_title('Mean Rating by Year', fontsize=13, fontweight='bold')\n","ax1.set_xlabel('Year')\n","ax1.set_ylabel('Mean Rating')\n","ax1.grid(True)\n","\n","# Set custom x-axis ticks and labels to display all years for the yearly plot\n","custom_xticks_yearly = mean_rating_by_year.index\n","ax1.set_xticks(custom_xticks_yearly)\n","ax1.set_xticklabels(custom_xticks_yearly, rotation=45)  # Rotating year labels for better readability\n","\n","# Annotate data points in the yearly plot\n","for x, y in zip(custom_xticks_yearly, mean_rating_by_year.values):\n","    ax1.annotate(f'{y:.2f}', (x, y), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n","\n","# Plot the mean rating by month\n","ax2.plot(mean_rating_by_month.index, mean_rating_by_month.values, marker='o', linestyle='-', color='g')\n","ax2.set_title('Mean Rating by Month', fontsize=13, fontweight='bold')\n","ax2.set_xlabel('Month')\n","ax2.set_ylabel('Mean Rating')\n","ax2.grid(True)\n","\n","# Set custom x-axis ticks and labels to display all 12 months for the monthly plot\n","custom_xticks_monthly = list(range(1, 13))  # Months 1 to 12\n","custom_xtick_labels_monthly = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n","ax2.set_xticks(custom_xticks_monthly)\n","ax2.set_xticklabels(custom_xtick_labels_monthly)  # Custom month labels\n","\n","# Annotate data points in the monthly plot\n","for x, y in zip(mean_rating_by_month.index, mean_rating_by_month.values):\n","    ax2.annotate(f'{y:.2f}', (x, y), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n","\n","# Adjust spacing between subplots\n","plt.tight_layout()\n","\n","# Show the plots\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"5xUNGNqajJcd"},"source":["The mean ratings based on two different time aggregations: by year and by month.\n","\n","Let's discuss and conclude the insights from this data:\n","\n","**Mean Rating by Year:**\n","- The data shows the mean ratings for each year from 2014 to 2023.\n","- In 2014, the mean rating was approximately 4.41, indicating a relatively positive sentiment in customer reviews.\n","- The mean rating increased in 2015 to approximately 4.54, suggesting an improvement in customer satisfaction.\n","- There was a slight decrease in the mean rating in 2016, dropping to around 4.42.\n","- Subsequently, there was a consistent increase in mean ratings from 2017 to 2018, reaching a peak of about 4.66.\n","- The mean rating remained relatively high in the following years, with fluctuations.\n","- In 2023, the mean rating reached approximately 4.58.\n","\n","**Mean Rating by Month:**\n","- This data provides the mean ratings for each month of the year.\n","- January (1) and December (12) had the highest mean ratings, with approximately 4.60 and 4.65, respectively. These months might correspond to periods of higher customer satisfaction.\n","- April (4) and October (10) had slightly lower mean ratings, around 4.58.\n","- The months showed some fluctuations, but generally, the mean ratings remained above 4.57 throughout the year.\n","\n","**Conclusions:**\n","- The mean rating by year provides insights into the overall customer satisfaction trends. While there were some fluctuations, it's evident that ratings improved from 2014 to 2018, remained relatively stable until 2022, and saw a slight increase in 2023.\n","- The mean rating by month offers a more granular view of customer satisfaction throughout the year. January and December had the highest ratings, possibly due to seasonal factors or holidays.\n","- The lowest ratings were observed in April and October, indicating periods when customers might be less satisfied.\n","\n","In both cases, this analysis provides valuable information about customer sentiment over time, which can be used for further investigation or decision-making, such as identifying areas for improvement or adjusting marketing strategies based on seasonal trends."]},{"cell_type":"markdown","metadata":{"id":"eA_xNiRVjJcd"},"source":["### Time Series Plot of Ratings Over Time:"]},{"cell_type":"markdown","metadata":{"id":"cXdB2y9PjJce"},"source":["\n","\n","If you want to visualize how ratings change over time, you can create a time series line chart."]},{"cell_type":"markdown","metadata":{"id":"v4k90RIgjJce"},"source":["This analyzed data includes customer ratings from March 2014 to October 2023, providing insights into customer sentiment and satisfaction trends. The objective is to inform data-driven decision-making and enhance our services to meet customer expectations.\n","\n","**Key Findings**\n","\n","1. **Consistent Positive Sentiment**: The data reveals a consistent positive sentiment among customers, with average ratings ranging between 4.2 and 4.7. This indicates a high level of overall satisfaction with our services.\n","\n","2. **Seasonal Variations**: Seasonal variations in ratings are observed, with some months showing higher average ratings and others lower. Identifying the factors contributing to these seasonal fluctuations can help optimize service delivery and resource allocation.\n","\n","3. **Annual Trends**: Over the years, annual trends are evident, with specific periods exhibiting higher or lower average ratings. Understanding these trends can inform strategic planning and resource allocation.\n","\n","4. **Data-Driven Decisions**: The dataset provides the opportunity to make data-driven decisions for service enhancement. Analyzing customer feedback and its correlation with ratings can guide improvements in specific areas of service delivery.\n","\n","5. **Competitive Advantage**: Maintaining consistently high ratings positions Freedom Debt Relief Company as a market leader. Leveraging customer satisfaction can attract new clients and bolster our competitive advantage.\n","\n","**Recommendations**\n","\n","- Further analysis is recommended to identify the specific drivers of seasonal and annual rating variations. This analysis should include examining customer feedback for insights.\n","- Consider conducting a root cause analysis to identify any specific areas of improvement that can lead to enhanced customer satisfaction.\n","\n","Finally, Freedom Debt Relief Company enjoys a positive reputation with consistently high customer ratings. Leveraging this reputation for continuous improvement is essential for maintaining a competitive advantage. By delving deeper into the drivers of rating variations and aligning service delivery with customer expectations, we can further solidify our position in the market.\n"]},{"cell_type":"markdown","metadata":{"id":"JAOMuvbcjJce"},"source":["### Histogram of Ratings:"]},{"cell_type":"markdown","metadata":{"id":"KOk2lTwpjJcf"},"source":["\n","\n","A histogram will show the distribution of ratings. You can see how many ratings fall into each category"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rWztfjk6jJcf","executionInfo":{"status":"aborted","timestamp":1698685602385,"user_tz":-180,"elapsed":8,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Set a Seaborn style for better aesthetics\n","sns.set(style=\"whitegrid\")\n","\n","# Create a bar plot of ratings\n","plt.figure(figsize=(8, 6))\n","rating_counts = df['Rating'].value_counts().sort_index()\n","ax = plt.bar(rating_counts.index, rating_counts, color='skyblue', edgecolor='black')\n","\n","# Set custom bar labels at the center of each bar\n","for bar in ax:\n","    bar_x = bar.get_x() + bar.get_width() / 2\n","    bar_height = bar.get_height()\n","    plt.text(bar_x, bar_height, str(int(bar_height)), ha='center', va='bottom', fontsize=10, fontweight='bold')\n","\n","plt.title('Distribution of Ratings', fontsize=14, fontweight='bold')\n","plt.xlabel('Rating', fontsize=12)\n","plt.ylabel('Count', fontsize=12)\n","\n","# Show the plot\n","plt.tight_layout()\n","plt.show()\n","\n","# Print information about the distribution of ratings\n","print(\"Distribution of Ratings:\")\n","print(rating_counts)\n"]},{"cell_type":"markdown","metadata":{"id":"toiFuThmjJcg"},"source":["The distribution of ratings, as provided in the histogram results, offers insights into how customers have rated a product or service.\n","\n","Here is a discussion and conclusion based on the given distribution:\n","\n","**Distribution of Ratings:**\n","- The data is divided into five rating categories: 1, 2, 3, 4, and 5.\n","- Each category represents the number of reviews with that particular rating.\n","- The highest number of reviews falls into the 5-star category, with a count of 30,639.\n","- The 4-star rating is the second most common, with 4,758 reviews.\n","- The 3-star rating has a count of 1,622.\n","- The 2-star rating is the least common among positive ratings, with only 694 reviews.\n","- The lowest rating, 1 star, has a count of 1,365.\n","\n","**Discussion:**\n","- The data clearly shows that the majority of customers have given positive ratings, with 5-star and 4-star ratings being the most prevalent. This indicates that a substantial portion of customers are satisfied with the product or service, as these ratings are above the median score (3).\n","- The distribution is right-skewed, meaning there are far more positive reviews than negative ones. This skew suggests that the product or service generally meets customer expectations or even exceeds them.\n","- The relatively low counts in the 1-star and 2-star categories suggest that only a minority of customers expressed strong dissatisfaction.\n","- The 3-star category, while not as common as 4-star and 5-star ratings, still has a notable number of reviews, indicating a moderate level of satisfaction.\n","\n","**Conclusion:**\n","The distribution of ratings reflects a generally positive sentiment among customers who have left reviews. The bulk of reviews fall into the higher rating categories, suggesting that the product or service is well-received. However, the existence of lower ratings (1 to 3 stars) indicates that there is room for improvement or that a subset of customers had less positive experiences.\n","\n","To draw more specific conclusions or make further decisions based on this data, it may be helpful to conduct sentiment analysis on the reviews themselves or to analyze other factors, such as the time of the reviews or the topics mentioned in the reviews. This additional context can provide a deeper understanding of customer feedback."]},{"cell_type":"markdown","metadata":{"id":"rD4dklJzjJcg"},"source":["## NLP"]},{"cell_type":"markdown","metadata":{"id":"R2lvfJeSjJch"},"source":["#### Concatenate Review Title and Review Text Columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qBNAJuQ2jJch","executionInfo":{"status":"aborted","timestamp":1698685602385,"user_tz":-180,"elapsed":7,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["columns_to_concat = ['Review Title', 'Review Text']\n","\n","# Concatenate the selected columns into a new column 'Concatenated_Text'\n","df['Concatenated_Text'] = df[columns_to_concat].apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1)"]},{"cell_type":"markdown","metadata":{"id":"Zz7uZjFAjJch"},"source":["#### Text Preprocessing\n","\n","Before performing NLP tasks, it's important to preprocess the text data, which typically involves removing stopwords, punctuation, and converting text to lowercase."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m4FP2vV3jJci","executionInfo":{"status":"aborted","timestamp":1698685602385,"user_tz":-180,"elapsed":7,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["def preprocess_text(text):\n","    if isinstance(text, str):  # Check if the value is a string\n","        # Remove emojis from the text and store them separately\n","        emojis = [c for c in text if c in emoji.UNICODE_EMOJI]\n","\n","        # Tokenize the text\n","        words = word_tokenize(text.lower())\n","\n","        # Remove stopwords and punctuation\n","        words = [word for word in words if word.isalnum() and word not in stopwords.words('english')]\n","\n","        # Lemmatize words\n","        lemmatizer = WordNetLemmatizer()\n","        words = [lemmatizer.lemmatize(word) for word in words]\n","\n","        # Combine words and emojis back into a preprocessed text\n","        preprocessed_text = ' '.join(words) + ' '.join(emojis)\n","        return preprocessed_text\n","    else:\n","        return ''  # Return an empty string for non-text values (NaN)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ZSEjgW9jJci","executionInfo":{"status":"aborted","timestamp":1698685602386,"user_tz":-180,"elapsed":7,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["def preprocess_text(text):\n","    if isinstance(text, str):  # Check if the value is a string\n","        # Remove emojis from the text\n","        text = re.sub(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U0001FB00-\\U0001FBFF\\U0001FC00-\\U0001FCFF\\U0001FD00-\\U0001FDFF\\U0001FE00-\\U0001FEFF\\U0001FF00-\\U0001FFFF]+', '', text)\n","\n","        # Tokenize the text\n","        words = word_tokenize(text.lower())\n","\n","        # Remove stopwords and punctuation\n","        words = [word for word in words if word.isalnum() and word not in stopwords.words('english')]\n","\n","        # Lemmatize words\n","        lemmatizer = WordNetLemmatizer()\n","        words = [lemmatizer.lemmatize(word) for word in words]\n","\n","        return ' '.join(words)\n","    else:\n","        return ''  # Return an empty string for non-text values (NaN)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RmSu29-4jJci","executionInfo":{"status":"aborted","timestamp":1698685602387,"user_tz":-180,"elapsed":8,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["df['Cleaned_Concatenated_Text'] = df['Concatenated_Text'].apply(preprocess_text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FnVJYPjejJcj","executionInfo":{"status":"aborted","timestamp":1698685602387,"user_tz":-180,"elapsed":8,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["# Count the number of items in each text item\n","df['Text_Item_Count'] = df['Cleaned_Concatenated_Text'].apply(lambda x: len(x.split()))\n","\n","# Display both the text and the count of text items\n","text_with_count = df[['Cleaned_Concatenated_Text', 'Text_Item_Count']]\n","\n","print(text_with_count)\n","\n","# Calculate the total number of text items across all rows\n","total_text_items = df['Text_Item_Count'].sum()\n","\n","# Display the total number of text items\n","print(f\"Total Text Items: {total_text_items}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XUnLaVL7jJcj","executionInfo":{"status":"aborted","timestamp":1698685602388,"user_tz":-180,"elapsed":9,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["all_words = ' '.join(df['Cleaned_Concatenated_Text']).split()\n","\n","# Count the occurrences of each word\n","word_counts = Counter(all_words)\n","\n","# Sort the word counts in descending order\n","sorted_word_counts = dict(sorted(word_counts.items(), key=lambda item: item[1], reverse=True))\n","\n","# Create a DataFrame for the first 50 most common words\n","word_counts_df = pd.DataFrame(list(sorted_word_counts.items()), columns=['Word', 'Count'])\n","\n","print(word_counts_df.head(30))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aytlANdpjJck","executionInfo":{"status":"aborted","timestamp":1698685602389,"user_tz":-180,"elapsed":10,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["top_n = 30\n","top_words = list(sorted_word_counts.keys())[:top_n]\n","top_word_counts = list(sorted_word_counts.values())[:top_n]\n","\n","plt.figure(figsize=(12, 6))\n","plt.barh(top_words, top_word_counts, color='skyblue')\n","plt.xlabel('Word Count', fontweight='bold')\n","plt.ylabel('Words', fontweight='bold')\n","plt.title(f'Top {top_n} Most Common Words', fontweight='bold')\n","plt.gca().invert_yaxis()\n","plt.tight_layout()\n","\n","for i, count in enumerate(top_word_counts):\n","    plt.text(count, i, str(count), va='center', fontsize=10, color='black')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"0tRiSgjcjJck"},"source":["### Sentiment Analysis\n","\n","Sentiment analysis helps determine the sentiment (positive, negative, or neutral) of text data. VADER or TextBlob libraries for sentiment analysis were used"]},{"cell_type":"markdown","metadata":{"id":"Ni5nDHkWjJcl"},"source":["\n","Analyzing the relationship between Ratings and Review Text can involve sentiment analysis to derive a sentiment score from the text and see if it aligns with the given Ratings.\n","Here's a step-by-step guide on how you can do this:"]},{"cell_type":"markdown","metadata":{"id":"7BGp83V7jJcl"},"source":["#### 1. Sentiment Analysis:\n","You can use a sentiment analysis tool or library to analyze the sentiment of the review text. There are various sentiment analysis libraries available in Python, such as NLTK, TextBlob, VADER, and spaCy. Choose one that you are comfortable with.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nD3cxDR2jJcm","executionInfo":{"status":"aborted","timestamp":1698685602390,"user_tz":-180,"elapsed":11,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["#For example, using TextBlob:\n","\n","from textblob import TextBlob\n","\n","def analyze_sentiment(Cleaned_Concatenated_Text):\n","    analysis = TextBlob(Cleaned_Concatenated_Text)\n","    # Classify sentiment as positive, negative, or neutral\n","    if analysis.sentiment.polarity > 0:\n","        return 'positive'\n","    elif analysis.sentiment.polarity < 0:\n","        return 'negative'\n","    else:\n","        return 'neutral'\n"]},{"cell_type":"markdown","metadata":{"id":"OErE6MS5jJcm"},"source":["#### 2. Assign Sentiment Scores:\n","You can assign numerical sentiment scores to each review based on the sentiment analysis. For example, you might assign a score of 5 for 'positive,' 3 for 'neutral,' and 1 for 'negative.'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sgO9cGI3jJcn","executionInfo":{"status":"aborted","timestamp":1698685602390,"user_tz":-180,"elapsed":11,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["def assign_sentiment_score(sentiment):\n","    if sentiment == 'positive':\n","        return 5\n","    elif sentiment == 'neutral':\n","        return 3\n","    else:\n","        return 1\n"]},{"cell_type":"markdown","metadata":{"id":"Rd-TFPp8jJcn"},"source":["#### 3. Calculate a New Rating:\n","Calculate a new rating for each review based on the sentiment score derived from the review text. You can simply average the sentiment scores from the reviews. This will give you a new rating for each review."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tqY5IlnGjJcn","executionInfo":{"status":"aborted","timestamp":1698685602391,"user_tz":-180,"elapsed":11,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["def calculate_new_rating(sentiment_score):\n","    # You can customize this formula as needed\n","    return (sentiment_score * 5) / 3"]},{"cell_type":"markdown","metadata":{"id":"vRNi3mmDjJco"},"source":["#### 4.Compare Given and Calculated Ratings:\n","Once you have the calculated ratings, you can compare them with the given ratings to assess how well the sentiment analysis aligns with the reviewer's direct ratings."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hvy2lT8qjJco","executionInfo":{"status":"aborted","timestamp":1698685602391,"user_tz":-180,"elapsed":12330,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["def compare_ratings(given_rating, calculated_rating):\n","    # Calculate the difference between given and calculated ratings\n","    rating_difference = given_rating - calculated_rating\n","    return rating_difference\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jf4DGQOzjJco","executionInfo":{"status":"aborted","timestamp":1698685602391,"user_tz":-180,"elapsed":12329,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["def analyze_sentiment(Cleaned_Concatenated_Text):\n","    analyzer = SentimentIntensityAnalyzer()\n","    sentiment_scores = analyzer.polarity_scores(Cleaned_Concatenated_Text)\n","    return sentiment_scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-PCYPgdojJcp","executionInfo":{"status":"aborted","timestamp":1698685602391,"user_tz":-180,"elapsed":12328,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["# Apply sentiment analysis to the 'Cleaned_Concatenated_Text' column\n","df['Sentiment_Scores'] = df['Cleaned_Concatenated_Text'].apply(analyze_sentiment)\n","\n","# Extract the sentiment labels (positive, negative, neutral)\n","df['Sentiment_Label'] = df['Sentiment_Scores'].apply(lambda x: 'positive' if x['compound'] > 0 else ('negative' if x['compound'] < 0 else 'neutral'))\n","\n","print(\"Sentiment Scores:\")\n","print(df['Sentiment_Scores'])\n","\n","print(\"\\nSentiment Labels:\")\n","print(df['Sentiment_Label'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3PeW6brKjJcp","executionInfo":{"status":"aborted","timestamp":1698685602392,"user_tz":-180,"elapsed":12328,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["# Get the counts of each sentiment label\n","sentiment_counts = df['Sentiment_Label'].value_counts()\n","\n","plt.figure(figsize=(8, 6))\n","sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values, hue=sentiment_counts.index, palette='viridis', legend=False)\n","plt.xlabel('Sentiment Label', weight='bold')\n","plt.ylabel('Count', weight='bold')\n","plt.title('Distribution of Sentiment Labels', fontsize=12, weight='bold')\n","plt.tight_layout()\n","\n","for i, count in enumerate(sentiment_counts.values):\n","    plt.text(i, count, str(count), ha='center', va='bottom', fontsize=10)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"K-VXOBgxjJcq"},"source":["#### Topic Modeling\n","\n","Topic modeling helps identify the main topics within a collection of documents. Latent Dirichlet Allocation (LDA) topic modeling using the Gensim library were used."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jpPe6gkYjJcq","executionInfo":{"status":"aborted","timestamp":1698685602392,"user_tz":-180,"elapsed":12327,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["# Tokenize and preprocess text (using the 'Cleaned_Concatenated_Text' column)\n","tokenized_reviews = [word_tokenize(text) for text in df['Cleaned_Concatenated_Text']]\n","\n","# Create a dictionary and corpus for LDA\n","dictionary = corpora.Dictionary(tokenized_reviews)\n","corpus = [dictionary.doc2bow(text) for text in tokenized_reviews]\n","\n","# Train an LDA model\n","lda_model = models.LdaModel(corpus, num_topics=5, id2word=dictionary, passes=15)\n","\n","# Get topics and associated words\n","topics = lda_model.print_topics(num_words=5)\n","\n","# Print the topics\n","for topic in topics:\n","    print(topic)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IORjixNMjJcr","executionInfo":{"status":"aborted","timestamp":1698685602392,"user_tz":-180,"elapsed":12326,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["analyzer = SentimentIntensityAnalyzer()\n","\n","# Define a function to classify reviews as positive, negative, or neutral\n","def classify_sentiment(text):\n","    sentiment_scores = analyzer.polarity_scores(text)\n","    compound_score = sentiment_scores['compound']\n","\n","    if compound_score >= 0.05:\n","        return 'Positive'\n","    elif compound_score <= -0.05:\n","        return 'Negative'\n","    else:\n","        return 'Neutral'\n","\n","# Apply sentiment classification to the 'Cleaned_Concatenated_Text' column\n","df['Sentiment_Label'] = df['Cleaned_Concatenated_Text'].apply(classify_sentiment)\n","\n","# Count the number of positive and negative reviews\n","positive_reviews_count = len(df[df['Sentiment_Label'] == 'Positive'])\n","negative_reviews_count = len(df[df['Sentiment_Label'] == 'Negative'])\n","\n","# Create a pie chart to visualize the distribution of positive and negative reviews\n","labels = ['Positive', 'Negative']\n","sizes = [positive_reviews_count, negative_reviews_count]\n","colors = ['green', 'red']\n","\n","plt.figure(figsize=(6, 6))\n","plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n","plt.title('Distribution of Positive and Negative Reviews')\n","plt.axis('equal')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"07Qb5F0PjJcr"},"source":["#### Sentiment Trends Over Time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KtkyMXnijJcr","executionInfo":{"status":"aborted","timestamp":1698685602392,"user_tz":-180,"elapsed":12326,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["# Group the data by date and sentiment label, and count the occurrences\n","sentiment_trends = df.groupby(['Review Date', 'Sentiment_Label']).size().unstack(fill_value=0)\n","\n","# Reset the index to have 'Date' as a column\n","sentiment_trends.reset_index(inplace=True)\n","\n","# Set 'Date' column as the index for the DataFrame\n","sentiment_trends.set_index('Review Date', inplace=True)\n","\n","# Plot the sentiment trends over time\n","plt.figure(figsize=(12, 6))\n","sns.lineplot(data=sentiment_trends, markers=True)\n","plt.title('Sentiment Trends Over Time', weight='bold')\n","plt.xlabel('Review Date', weight='bold')\n","plt.ylabel('Count', weight='bold')\n","plt.legend(title='Sentiment Label', loc='upper right')\n","plt.grid(True)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2JmOH4SyjJcs","executionInfo":{"status":"aborted","timestamp":1698685602392,"user_tz":-180,"elapsed":12325,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["# Group the data by date and sentiment label, and count the occurrences\n","sentiment_trends = df.groupby(['Review Date', 'Sentiment_Label']).size().unstack(fill_value=0)\n","\n","# Reset the index to have 'Date' as a column\n","sentiment_trends.reset_index(inplace=True)\n","\n","# Set 'Date' column as the index for the DataFrame\n","sentiment_trends.set_index('Review Date', inplace=True)\n","\n","# Filter the DataFrame for only the \"Negative\" sentiment label\n","negative_sentiment_trends = sentiment_trends['Negative']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SMpNE7P9jJcs","executionInfo":{"status":"aborted","timestamp":1698685602393,"user_tz":-180,"elapsed":12325,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["# Find the date with the highest negative sentiment count\n","max_negative_date = negative_sentiment_trends.idxmax()\n","max_negative_count = negative_sentiment_trends.max()\n","\n","# Plot the negative sentiment trends over time\n","plt.figure(figsize=(12, 6))\n","sns.lineplot(data=negative_sentiment_trends, markers=True)\n","plt.title('Negative Sentiment Trends Over Time', weight='bold')\n","plt.xlabel('Review Date', weight='bold')\n","plt.ylabel('Count', weight='bold')\n","plt.grid(True)\n","\n","# Add text annotation for the top negative value\n","plt.text(max_negative_date, max_negative_count, f'Max Negative: {max_negative_count}', ha='right', va='bottom', fontsize=12)\n","\n","plt.show()\n","\n","# Print the top negative value and date\n","print(f\"Top Negative Value: {max_negative_count} on {max_negative_date}\")"]},{"cell_type":"markdown","metadata":{"id":"dYot14LBjJcs"},"source":["#### Deep Learning Model"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"Zd0nVvFGjJct","executionInfo":{"status":"error","timestamp":1698687311903,"user_tz":-180,"elapsed":9,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}},"outputId":"2ab30ceb-7823-4057-cf48-113caaccfe80"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-b7463560e70b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Extract text data and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtext_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Review Text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Rating\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Load the BERT tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}],"source":["# Extract text data and labels\n","text_data = df[\"Review Text\"]\n","labels = df[\"Rating\"]\n","\n","# Load the BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","\n","# Tokenize and pad the text data\n","max_seq_length = 64  # Specify your desired sequence length"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0BPE6jy4jJct","executionInfo":{"status":"aborted","timestamp":1698685602393,"user_tz":-180,"elapsed":12322,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["# Group reviews by their length (number of tokens)\n","grouped_reviews = {}\n","for idx, text in enumerate(text_data):\n","    encoded_dict = tokenizer.encode_plus(\n","        text,\n","        add_special_tokens=True,\n","        max_length=max_seq_length,\n","        return_attention_mask=True,\n","        return_tensors='pt',\n","        truncation=True  # Explicitly truncate examples to max length\n","    )\n","\n","    review_length = encoded_dict['input_ids'].size(1)\n","    if review_length not in grouped_reviews:\n","        grouped_reviews[review_length] = {\n","            'input_ids': [],\n","            'attention_mask': []\n","        }\n","\n","    grouped_reviews[review_length]['input_ids'].append(encoded_dict['input_ids'])\n","    grouped_reviews[review_length]['attention_mask'].append(encoded_dict['attention_mask'])\n","\n","# Create DataLoaders\n","data_loaders = []\n","\n","for length, group in grouped_reviews.items():\n","    num_samples = len(group['input_ids'])\n","    input_ids_subset = torch.cat(group['input_ids'], dim=0)\n","    attention_masks_subset = torch.cat(group['attention_mask'], dim=0)\n","    labels_subset = torch.tensor(labels.values[:num_samples] - 1, dtype=torch.long)  # Adjust labels\n","\n","    dataset = TensorDataset(input_ids_subset, attention_masks_subset, labels_subset)\n","    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","    data_loaders.append(data_loader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tZAoI5T3jJct","executionInfo":{"status":"aborted","timestamp":1698685602393,"user_tz":-180,"elapsed":12317,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["# Load the BERT model\n","model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LYFjeC96jJcu","executionInfo":{"status":"aborted","timestamp":1698685602394,"user_tz":-180,"elapsed":12314,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["# Set training hyperparameters\n","epochs = 3\n","optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Move the model to the device (CPU or GPU)\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0A95imxkjJcv","executionInfo":{"status":"aborted","timestamp":1698685602394,"user_tz":-180,"elapsed":12313,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["# Training loop\n","for epoch in range(epochs):\n","    model.train()\n","    for batch in train_dataloader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        # Adjust labels to be 0-indexed\n","        labels = labels - 1\n","        labels = labels.to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s2rt7XXVjJcv","executionInfo":{"status":"aborted","timestamp":1698685602394,"user_tz":-180,"elapsed":12312,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["# Evaluation\n","model.eval()\n","val_loss = 0\n","val_preds = []\n","with torch.no_with():\n","    for batch in val_dataloader:\n","        input_ids, attention_mask, labels = batch\n","        input_ids = input_ids.to(device)\n","        attention_mask = attention_mask.to(device)\n","        labels = labels.to(device)\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        val_loss += outputs.loss.item()\n","        logits = outputs.logits\n","        preds = torch.argmax(logits, dim=1)\n","        val_preds.extend(preds.cpu().numpy())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rXCrq_9cjJcv","executionInfo":{"status":"aborted","timestamp":1698685602395,"user_tz":-180,"elapsed":12312,"user":{"displayName":"Arif Aygun","userId":"14709098776997350988"}}},"outputs":[],"source":["# Calculate validation loss and print classification report\n","val_loss /= len(val_dataloader)\n","print(f\"Validation Loss: {val_loss}\")\n","target_names = ['Rating 1', 'Rating 2', 'Rating3', 'Rating 4', 'Rating 5']\n","print(classification_report(val_labels, val_preds, target_names=target_names))\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"accelerator":"TPU"},"nbformat":4,"nbformat_minor":0}